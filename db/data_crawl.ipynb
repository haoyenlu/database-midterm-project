{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import requests\n",
    "import json\n",
    "import googlemaps\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pinyin\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from googlemaps import *\n",
    "from os.path import join, dirname\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "\n",
    "#google api key for GEOCODING and PLACES\n",
    "API_KEY = input(\"Google API key:\")\n",
    "\n",
    "'''\n",
    "## 爬取店家資訊的主程式\n",
    "\n",
    "cities = [\"臺北市\", \"新北市\", \"桃園市\", \"臺中市\", \"臺南市\", \"高雄市\", \"基隆市\",\n",
    "         \"新竹市\", \"嘉義市\", \"新竹縣\", \"苗栗縣\", \"彰化縣\", \"雲林縣\",\n",
    "         \"嘉義縣\", \"屏東縣\", \"宜蘭縣\", \"花蓮縣\", \"臺東縣\", \"澎湖縣\"]\n",
    "\n",
    "ids = []\n",
    "\n",
    "for city in cities:\n",
    "    geocode_result = gmaps.geocode(city)\n",
    "    loc = geocode_result[0]['geometry']['location']\n",
    "    \n",
    "    for place in gmaps.places_nearby(keyword = \"海灘\", location = loc, radius = 50000)['results']:\n",
    "        ids.append(place['place_id'])\n",
    "        \n",
    "ids = list(set(ids))\n",
    "info = []\n",
    "for each_id in ids:\n",
    "    info.append(gmaps.place(place_id = each_id, language = 'zh-TW')['result'])\n",
    "    \n",
    "final_data = pd.DataFrame.from_dict(info)\n",
    "    \n",
    "# 我需要找的東西有：名字、經度、緯度、他在哪一個城鎮\n",
    "\n",
    "allowed = ['沙灘', '海岸', '遊憩區', '浴場', '海濱', '灣', '海灘']\n",
    "final_list = list(final_data.to_numpy())\n",
    "\n",
    "sort_out = []\n",
    "counter = 0\n",
    "\n",
    "for line in final_list:\n",
    "    containing = False\n",
    "    if line[0][0]['long_name'] == '台灣':\n",
    "        continue\n",
    "    for i in allowed:\n",
    "        if i in line[0][0]['long_name']:\n",
    "            containing = True\n",
    "            break\n",
    "    if containing:\n",
    "        town = ''\n",
    "        county = ''\n",
    "        try:\n",
    "            for element in line[0]:\n",
    "                if element['types'][0] == 'administrative_area_level_3':\n",
    "                    town = element['long_name']\n",
    "                elif element['types'][0] == 'administrative_area_level_2' or element['types'][0] == 'administrative_area_level_1':\n",
    "                    county = element['long_name']\n",
    "                    \n",
    "            if town == '' or county == '':\n",
    "                references = []\n",
    "                get = gmaps.places_nearby(location = [line[5]['location']['lng'], line[5]['location']['lat']], radius = 100)['results']\n",
    "                references.append(gmaps.place(place_id = get[i]['place_id'], language = 'zh-TW')['result']['address_component'] for i in range(2))\n",
    "                for reference in references:\n",
    "                    for ref_element in reference[0]:\n",
    "                        if ref_element['types'][0] == 'administrative_area_level_3':\n",
    "                            town = element['long_name']\n",
    "                        elif ref_element['types'][0] == 'administrative_area_level_2' or ref_element['types'][0] == 'administrative_area_level_1':\n",
    "                            county = element['long_name']\n",
    "                        \n",
    "                    \n",
    "                \n",
    "            new_line = [line[0][0]['long_name'], line[5]['location']['lng'], line[5]['location']['lat'],\n",
    "                        town, county]\n",
    "            sort_out.append(new_line)\n",
    "            counter += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "#beach_big = list(sort_out)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#確認spots.csv在同一目錄下\n",
    "#讀取非浪點資料\n",
    "beach94 = []\n",
    "\n",
    "with open('./spots.csv', newline='',encoding=\"utf-8\") as csvfile:\n",
    "    \n",
    "    rows = csv.reader(csvfile)\n",
    "    firstRow = True;    \n",
    "    for row in rows:\n",
    "        if firstRow == True:\n",
    "            firstRow = False\n",
    "            continue\n",
    "        beach94.append(row)\n",
    "\n",
    "print(\"loading spots.csv complete\")\n",
    "\n",
    "#取得浪點LIST\n",
    "url = 'https://swelleye.com/surf-forecasts/'\n",
    "resp = requests.get(url)\n",
    "resp.encoding='utf-8' \n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "#浪點英文名\n",
    "spot_list = []\n",
    "#浪點中文名\n",
    "chi_list = []\n",
    "\n",
    "for i in soup.find_all(\"a\", class_=\"list-link\"):\n",
    "    name_href = i['href']\n",
    "    chi_name = i.text\n",
    "    \n",
    "    name = name_href[12:(len(name_href)-14)]\n",
    "    spot_list.append(name)\n",
    "    chi_list.append(chi_name[29:(len(chi_name)-25)])\n",
    "print(\"loading surf spot complete\")\n",
    "    \n",
    "#取得浪點經緯度\n",
    "lon_list = []\n",
    "lat_list = []\n",
    "\n",
    "for spot_name in spot_list:\n",
    "    spot_url = \"https://swelleye.com/surf-spots/\" + spot_name + \"/#surf-forecast\"\n",
    "    resp = requests.get(spot_url)\n",
    "    resp.encoding='utf-8' \n",
    "    soup = BeautifulSoup(resp.text,\"html.parser\")\n",
    "    geo_url = soup.find(\"iframe\", height=\"570px\")['src']\n",
    "    lat = float( geo_url[(geo_url.find('=')+1):geo_url.find('&')] )\n",
    "    lon = float( geo_url[(geo_url.find( '=', geo_url.find('=')+1 )+1):geo_url.find('&', geo_url.find('&')+1)] )\n",
    "    lat_list.append(lat)\n",
    "    lon_list.append(lon)\n",
    "print(\"loading lat, lon complete\")\n",
    "    \n",
    "#取得浪點所在鄉鎮資料\n",
    "spot_town = []\n",
    "\n",
    "key = API_KEY\n",
    "for i in range(len(spot_list)):\n",
    "    resp = requests.get('http://api.opencube.tw/location?lat='+str(lat_list[i])+'&lng='+str(lon_list[i])+'&key='+key)\n",
    "    rjson = json.loads(resp.text)\n",
    "    spot_town.append(rjson[\"data\"][\"district\"])\n",
    "print(\"loading loc town complete\")\n",
    "\n",
    "#取得縣市list\n",
    "city_url = \"https://api.nlsc.gov.tw/other/ListCounty\"\n",
    "resp = requests.get(city_url)\n",
    "soup = BeautifulSoup(resp.text,\"html.parser\")\n",
    "city_list = []\n",
    "city_id = []\n",
    "for i in soup.find_all('countyname'):\n",
    "    city_list.append(i.text)\n",
    "for i in soup.find_all('countycode'):\n",
    "    city_id.append(i.text)\n",
    "\n",
    "#取得鄉鎮list\n",
    "town_list = []\n",
    "town_id = []\n",
    "town_loc_city = []\n",
    "for i in range(len(city_id)):\n",
    "    town_url = \"https://api.nlsc.gov.tw/other/ListTown/\" + city_id[i]\n",
    "    resp = requests.get(town_url)\n",
    "    soup = BeautifulSoup(resp.text,\"html.parser\")\n",
    "    for k in soup.find_all('townname'):\n",
    "        town_list.append(k.text)\n",
    "    for j in soup.find_all('towncode'):\n",
    "        town_id.append(j.text)\n",
    "        town_loc_city.append(city_list[i])\n",
    "print(\"loading town list complete\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得商店資訊\n",
    "\n",
    "gmaps = googlemaps.Client(key = API_KEY)\n",
    "\n",
    "shop_name_list = []\n",
    "shop_spot_list = []\n",
    "shop_rating_list = []\n",
    "shop_open_list = []\n",
    "shop_address_list = []\n",
    "\n",
    "for i in range(len(spot_list)):\n",
    "    loc = {'lat':lat_list[i], 'lng':lon_list[i]}\n",
    "    output = gmaps.places_nearby(keyword=\"衝浪\", location = loc, radius = 500)['results']\n",
    "    for j in range(len(output)):\n",
    "        if(j<5):\n",
    "            try:\n",
    "                shop_name_list.append(output[j]['name'])\n",
    "                shop_rating_list.append(output[j]['rating'])\n",
    "                shop_address_list.append(output[j]['vicinity'])\n",
    "                if('opening_hours' in output[j].keys()):\n",
    "                    if('open_now' in output[j]['opening_hours'].keys()):\n",
    "                        shop_open_list.append(output[j]['opening_hours']['open_now'])\n",
    "                    else:\n",
    "                        shop_open_list.append(None)\n",
    "                else:\n",
    "                    shop_open_list.append(None)\n",
    "                shop_spot_list.append(i+1)\n",
    "            except:\n",
    "                continue\n",
    "print(\"loading shop list complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得新聞資料\n",
    "news_title_list = []\n",
    "news_url_list = []\n",
    "news_spot_list = []\n",
    "news_time_list = []\n",
    "\n",
    "for i in range(len(chi_list)):\n",
    "    url = 'https://news.google.com/search?q='+chi_list[i]+'衝浪&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant'\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding='utf-8' \n",
    "    soup = BeautifulSoup(resp.text,\"html.parser\")\n",
    "    for j in soup.find_all(\"a\", class_=\"DY5T1d RZIKme\")[0:5]:\n",
    "        \n",
    "        news_title_list.append(j.text)\n",
    "        news_spot_list.append(i+1)\n",
    "        url = 'https://news.google.com'+ j['href'][1:]\n",
    "        news_url_list.append(url)\n",
    "        \n",
    "        resp = requests.get(url)\n",
    "        resp.encoding='utf-8' \n",
    "        if(soup.find(\"time\")):\n",
    "            k = soup.find(\"time\")\n",
    "            news_time_list.append(str(k.text).replace(' ', ''))\n",
    "        else:\n",
    "            news_time_list.append(\"\")\n",
    "print(\"loading news list complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d14db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#獲取浪點預報\n",
    "print(\"crawling using selelium auto click, please do not click the operating browser or press tab\")\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "info_spot_id = []\n",
    "info_date = []\n",
    "wave_height = []\n",
    "wave_period = []\n",
    "wave_dir = []\n",
    "wind_speed = []\n",
    "wind_dir = []\n",
    "temp = []\n",
    "sea_temp = []\n",
    "\n",
    "today = datetime.today()\n",
    "for i in range(len(spot_list)):\n",
    "    url = \"https://swelleye.com/surf-spots/\" + spot_list[i] + \"/#surf-forecast\"\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    xf = driver.find_element(By.XPATH, '//*[@id=\"surf-forecast\"]/div[2]/div[1]/iframe')\n",
    "    driver.switch_to.frame(xf)\n",
    "    #7天預報\n",
    "    #print(spot_list[i])\n",
    "    for j in range(6):\n",
    "        while (1):\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, '//*[@id=\"forecast\"]/div[1]/a[' + str(j+1) + ']/div[1]/div[1]').click()\n",
    "                break\n",
    "            except:\n",
    "                print(\"position failed, auto retry\")\n",
    "        #12個時段\n",
    "        for k in range(11):\n",
    "            #浪點\n",
    "            info_spot_id.append(i+1)\n",
    "            pred_day = today + timedelta(days=j)\n",
    "            info_date.append(pred_day.strftime('%Y-%m-%d') + f\" {k*2:02d}\")\n",
    "            #浪高(m)\n",
    "            wave_height.append(driver.find_element(By.XPATH, '//*[@id=\"forecast-content\"]/div[' + str(j+1) + ']/div/div[2]/div[' + str(k+1) + ']/span[1]').text)\n",
    "            #浪週期(s)\n",
    "            wave_period.append(driver.find_element(By.XPATH, '//*[@id=\"forecast-content\"]/div[' + str(j+1) + ']/div/div[3]/span[' + str(k+1) + ']').text)\n",
    "            #浪向(。)\n",
    "            wave_dir.append( re.sub(\"[^0-9]\", \"\",str(driver.find_element(By.XPATH, '//*[@id=\"forecast-content\"]/div[' + str(j+1) + ']/div/div[4]/div[' + str(k+1) + ']/div/div').get_attribute('class'))) )\n",
    "            #風力(m/s)\n",
    "            wind_speed.append(driver.find_element(By.XPATH, '//*[@id=\"forecast-content\"]/div[' + str(j+1) + ']/div/div[5]/div[' + str(k+1) + ']/span[1]').text)\n",
    "            #風向(。)\n",
    "            wind_dir.append( re.sub(\"[^0-9]\", \"\",str(driver.find_element(By.XPATH, '//*[@id=\"forecast-content\"]/div[' + str(j+1) + ']/div/div[6]/div[' + str(k+1) + ']/div/div').get_attribute('class'))) )\n",
    "            #溫度(。C)\n",
    "            temp.append(driver.find_element(By.XPATH, '//*[@id=\"forecast-content\"]/div[' + str(j+1) + ']/div/div[9]/div[' + str(k+1) + ']/span[1]').text)\n",
    "            #海溫(。C)\n",
    "            sea_temp.append(driver.find_element(By.XPATH, '//*[@id=\"forecast-content\"]/div[' + str(j+1) + ']/div/div[10]/div[' + str(k+1) + ']/span[1]').text)   \n",
    "    driver.switch_to.default_content()\n",
    "driver.quit()\n",
    "print(\"loading info list complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ab73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Town Table\n",
    "df_town = pd.DataFrame(list(zip(town_id, town_list,town_loc_city)), columns =['Id','Name', 'City'])\n",
    "df_town['Name'] = df_town['Name'].str.replace('臺','台')\n",
    "df_town['City'] = df_town['City'].str.replace('臺','台')\n",
    "\n",
    "#Type Table\n",
    "df_type = pd.DataFrame([[1, '浪點'], [2, '海水浴場'], [3, '海灘'], [4, '其他']], columns = ['Id', 'Name'])\n",
    "\n",
    "#Spot Table\n",
    "type_list = [1]*len(chi_list)\n",
    "spot_town_id = []\n",
    "other_town_id = []\n",
    "for i in spot_town:\n",
    "    if i is None:\n",
    "        spot_town_id.append(None)\n",
    "    else:\n",
    "        spot_town_id.append(df_town.loc[df_town.Name == i, 'Id'].values[0])\n",
    "\n",
    "df_other = pd.DataFrame(beach94, columns = ['Name', 'Lon', 'Lat', 'Town', 'County', 'index'])\n",
    "for i in df_other.itertuples(index=False):\n",
    "    value_id = df_town.loc[df_town.Name == i.Town, 'Id'].values\n",
    "    if(len(value_id) == 0):\n",
    "        other_town_id.append(None)\n",
    "    elif(len(value_id) == 1):\n",
    "        other_town_id.append(value_id[0])\n",
    "    else:\n",
    "        found = False\n",
    "        for j in value_id:\n",
    "            if(df_town.loc[df_town.Id == j, 'City'].values[0] == i.County):\n",
    "                other_town_id.append(j)\n",
    "                found = True\n",
    "                break\n",
    "        if(found == False):\n",
    "            other_town_id.append(None)\n",
    "type_other = []\n",
    "for i in beach94:\n",
    "    if i:\n",
    "        if '灘' in i[0]:\n",
    "            type_other.append(3)\n",
    "        elif '浴場' in i[0]:\n",
    "            type_other.append(2)\n",
    "        else:\n",
    "            type_other.append(4)\n",
    "df_other['Town_id'] = other_town_id\n",
    "df_other['Type_id'] = type_other\n",
    "\n",
    "df_spot = pd.DataFrame(list(zip(chi_list, lon_list,lat_list,spot_town_id, type_list)), columns =['Name', 'Lon', 'Lat', 'Town_id', 'Type_id'])\n",
    "df_other2 = df_other[['Name', 'Lon', 'Lat', 'Town_id', 'Type_id']]\n",
    "df_spot = df_spot.append(df_other2, ignore_index = True)\n",
    "df_spot['Id'] = df_spot.index+1\n",
    "\n",
    "#Surfshop Table\n",
    "df_surfshop = pd.DataFrame(list(zip(shop_name_list, shop_address_list,shop_spot_list,shop_rating_list, shop_open_list)), columns =['Name', 'Address', 'Spot_id', 'Rating', 'Operating_now'])\n",
    "df_surfshop['Id'] = df_surfshop.index+1\n",
    "\n",
    "#News Table\n",
    "df_news = pd.DataFrame(list(zip(news_time_list, news_url_list,news_title_list,news_spot_list)), columns =['Date', 'Url', 'Title', 'Spot_id'])\n",
    "df_news['Id'] = df_news.index+1\n",
    "\n",
    "#Information Table\n",
    "df_information = pd.DataFrame(list(zip(info_spot_id, info_date,\n",
    "                                       wave_height, wave_period, wave_dir,\n",
    "                                       wind_speed, wind_dir,\n",
    "                                       temp, sea_temp)), columns =['Spot_id', 'Date', 'Wave_height', 'Wave_period', 'Wave_direction', 'Wind_speed', 'Wind_direction', 'Temperature', 'Sea_temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 Google API 進行評論爬蟲\n",
    "\n",
    "total_reviewers = []\n",
    "total_reviews = []\n",
    "\n",
    "for place in df_spot.itertuples(index=False):\n",
    "    geocode_result = gmaps.geocode(place.Name)\n",
    "    loc = geocode_result[0]['place_id']\n",
    "    try:\n",
    "        reference_review = gmaps.place(place_id = loc, language = 'zh-TW')['result']['reviews']\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                review = []\n",
    "                reviewer = []\n",
    "                review.append(place.Id)\n",
    "                review.append(reference_review[i]['author_name'])\n",
    "                review.append(reference_review[i]['rating'])\n",
    "                review.append(reference_review[i]['text'])\n",
    "                review.append(reference_review[i]['time'])\n",
    "\n",
    "                reviewer.append(reference_review[i]['author_name'])\n",
    "                reviewer.append(pinyin.get(reference_review[i]['author_name'], format = \"numerical\") + \"@gmail.com\")\n",
    "                total_reviewers.append(reviewer)\n",
    "                total_reviews.append(review)\n",
    "            except:\n",
    "                break\n",
    "    except:\n",
    "        continue\n",
    "print(\"loading review & reviewer list complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89665db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviewer Table\n",
    "df_reviewer = pd.DataFrame(total_reviewers, columns =['Name', 'Email'])\n",
    "df_reviewer = df_reviewer.drop_duplicates()\n",
    "df_reviewer['Id'] = df_reviewer.index+1\n",
    "\n",
    "#Recommend Table\n",
    "df_recommend = pd.DataFrame(total_reviews, columns =['Spot_id', 'Reviewer', 'Score', 'Content', 'Date'])\n",
    "rec_reviewer_id = []\n",
    "for i in df_recommend.Reviewer:\n",
    "    rec_reviewer_id.append(df_reviewer.loc[df_reviewer.Name == i, 'Id'].values[0])\n",
    "df_recommend['Reviewer_id'] = rec_reviewer_id\n",
    "rec_timestamp = df_recommend['Date']\n",
    "rec_dt = map(lambda x: (datetime.fromtimestamp(x)).strftime('%Y-%m-%d'), rec_timestamp)\n",
    "df_recommend['Date'] = list(rec_dt)\n",
    "df_recommend.drop(columns=[\"Reviewer\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7dfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your db account info\n",
    "username = input(\"Input Username:\")\n",
    "password = input(\"InputPassword:\")\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "   database=\"postgres\", user=username, password=password, host='localhost', port= '5432'\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "sql = 'CREATE DATABASE \"SURF\"';\n",
    "cursor.execute(sql)\n",
    "print(\"Database created successfully........\")\n",
    "conn.close()\n",
    "conn = psycopg2.connect(\n",
    "   database=\"SURF\", user=username, password=password, host='localhost', port= '5432'\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "sql = 'CREATE SCHEMA IF NOT EXISTS \"SURF\"';\n",
    "cursor.execute(sql)\n",
    "print(\"Schema created successfully........\")\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n",
    "engine = create_engine(\"postgresql://\" + username + \":\" + password + \"@localhost:5432/SURF\")\n",
    "df_type.to_sql('TYPE', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "df_town.to_sql('TOWN', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "df_spot.to_sql('SPOT', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "df_surfshop.to_sql('SURFSHOP', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "df_information.to_sql('INFORMATION', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "df_news.to_sql('NEWS', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "df_reviewer.to_sql('REVIEWER', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "df_recommend.to_sql('RECOMMEND', engine, schema = 'SURF', if_exists=\"append\", index = False)\n",
    "\n",
    "print(\"appending data complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d8fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
